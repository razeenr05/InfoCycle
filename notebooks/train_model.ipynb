{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae67a42b",
   "metadata": {},
   "source": [
    "# InfoCycle â€” Model Training  \n",
    "\n",
    "**Author:** Razeen Rahman  \n",
    "\n",
    "**Description:**  \n",
    "\n",
    "This notebook trains a ResNet18 deep learning model to classify recyclable materials (e.g., plastic, paper, glass, etc.) using the Garbage Classification dataset.\n",
    "\n",
    "The data set can be found here: https://www.kaggle.com/datasets/mostafaabla/garbage-classification? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfdc92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS GPU\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "# imports and device setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# device config (supports CUDA, MPS, and CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon MPS GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d659f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "data_dir = \"../datasets/garbage_classification\"\n",
    "\n",
    "train_dir = os.path.join(data_dir)\n",
    "test_dir = os.path.join(data_dir)\n",
    "\n",
    "# transforms for training & testing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892cf410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
      "Train size: 12412, Test size: 3103\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# set dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# 80% for training, 20% for testing\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# create train and test datasets\n",
    "splits = random_split(full_dataset, [train_size, test_size])\n",
    "train_dataset = splits[0]\n",
    "test_dataset = splits[1]\n",
    "\n",
    "# remove the transforms from test dataset\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(f\"Train size: {train_size}, Test size: {test_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2088768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 images at a time\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e97b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# unfreeze layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# replace final layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(class_names))\n",
    "\n",
    "# saved model\n",
    "model.load_state_dict(torch.load(\"../model/infocycle_v1.pth\"))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=2):\n",
    "    #training mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device) \n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            outputs = model(images) # forward pass\n",
    "            loss = criterion(outputs, labels) # compute loss\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update weights\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {running_loss:.4f}\")\n",
    "    \n",
    "#run training\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba29b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.035127296165%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    # evalulation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #get model prediction/output\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # ignore the value\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # increment counters\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:}%\")\n",
    "\n",
    "# run evaluation\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4dc427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../model/infocycle_v1.pth\n"
     ]
    }
   ],
   "source": [
    "# path\n",
    "save_path = \"../model/infocycle_v1.pth\"\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
